発表メモ
目的：益子先輩のノートを、AI-OCRを用いて電子化すること。
やったこと
①　ノートから文字のまとまりを検出し、切り抜いて保存する手法の考案
②　益子先輩の手書き文字を学習させたAI-OCRを作る。
③　学習させたモデルにノートを読ませ、出力結果をjsonファイルに保存する。

②や③のとき、ノートから、文字のまとまりを切り抜いて学習/読み取りをさせる必要がある。
しかし、いちいち文字のまとまりを手作業で切り抜くのは面倒である。→①を行う。

①
最初に、ノートから、処理を行いたい部分を切り出す。
6_new_0.jpg
縦に文字列のまとまりを切り抜く。
6_new_0_processed.tif
各文字列を切り抜く。
6_new_0_0_cropped.tif

ノートの特徴：
縦方向に文字列が並んでいる。
グレースケールにした時、手書き文字の部分は、罫線やノイズの部分より濃くなっている（罫線が手書き文字と同じくらい濃い場合もある）。

まず、グレースケールにして、色の濃い部分のみ抽出する。
# binary.jpg
ここで、抽出した部分に罫線が入っていた場合にミスがないよう、罫線除去を行う。
抽出した部分の輪郭を描く。
# edges.jpg
輪郭を膨張処理して、できたまとまりをつなげて大きなまとまりにする。
# dilated.jpg
縦方向にまとまりが集中しているエリアの周りに矩形を描き、切り抜く。
# 6_new_0_processed.tif

切り抜いた画像についても、各文字列の抽出を行う。罫線を削除すると、文字列に罫線が重なっている場合に文字列の重要な部分が消えてうまく抽出できないため（罫線に挟まれた上と下で分離したりする。）、罫線の除去は行わなかった。
# 19_new_0_3_7.tif

まず、輪郭を描き、
# 3edges.jpg
輪郭を膨張処理して、
# 3dilated.jpg
横方向にまとまりが集中しているエリアの周りに矩形を描き、切り抜く。
# 6_new_0_3_cropped.tif

②学習過程（jpn_bestをSTART_MODELとした追加学習）
tesseract-ocrを用いた。
チューニングを行わない場合、ほとんどの画像の読み取りができなかった。これは、tesseract-ocrが手書き文字読み取り用のocrでないためだと考えられる。
jpn_bestをSTART_MODELとした追加学習を行った（スクラッチングより精度が高くなったため）。
切り抜いた画像データ(.tif)と、その読み方(.gt.txt)のセットを、2000個用意する。
学習過程で工夫したこと
①画像解像度を300dpiに統一する。
Tesseract works best on images which have a DPI of at least 300 dpi.(https://tesseract-ocr.github.io/tessdoc/ImproveQuality)
②デフォルトの設定では、学習が早く終わりすぎてしまうため、学習率をデフォルト設定より下げた。
LERNING_RATE = 0.00001
③画像データの水増し
オリジナルの画像データ（2000枚）のほかに、射影変換、ノイズ添加による水増しを行ったものを加えた、合計6000枚の画像データで学習。
例
# 1_new_0_0_2_finished.tif
# 1_new_0_0_2_misalign_1.tif
# 1_new_0_0_2_finished_noise.tif
④wordlistの設定
It is also possible to add words to the word list Tesseract uses to help recognition, or to add common character patterns, which can further help to improve accuracy if you have a good idea of the sort of input you expect.(https://tesseract-ocr.github.io/tessdoc/ImproveQuality)
出てくる頻度の高い文字列を、wordlistに入れて学習させる。今回の場合、Ei、Ei+1や10:00Ei+1など。
⑤時刻を別に学習させる
切り抜いた時、時刻と種名+記号+個体数（例えば、# 1_new_0_0_6_finished.tif）が一緒に切り抜かれる。
そのまま学習させた場合、テストの段階において
種名+記号+個体数の組み合わせは、同じ読みのものが多いため、よく読めている。
しかし、時刻+種名+記号+個体数は、同じ読みの組み合わせが少なく、時刻がうまく読めない傾向にあった。
時刻と、種名+記号+個体数を、別で学習させることで、時刻がよりうまく読み取れるようになるのではないか

出力
1冊のノートを1枚のjsonファイルにまとめる。
# 厚切りジェイソン.png
{
"book_num"：,
"page_1":{
"content"：[
{"read": 読み取った結果,
"x_s": 読み取った結果に接する長方形の、左上のx座標,
"x_e": 読み取った結果に接する長方形の、左上のy座標,
"y_s": 読み取った結果に接する長方形の、右下のx座標,
"y_e": 読み取った結果に接する長方形の、右下のy座標
},{

}],
"memo":{
各ページに書いてある、メモの部分。ここは後で手動で書き込むため、空けてある。
}
}
"page_2":{

}
}

今後の展望
